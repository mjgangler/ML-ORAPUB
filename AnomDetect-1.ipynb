{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "genuine-settlement",
   "metadata": {},
   "source": [
    "# Anomaly Detection System (ML1)\n",
    "(c)2021 OraPub, Inc - All rights reserved.\n",
    "WARNING: Use at your own risk.\n",
    "\n",
    "For what has changed in this version, see the what-has-changed.txt document.\n",
    "\n",
    "Check DB Connection and Query: $ python ./AnomDetect-1.py db\n",
    "\n",
    "Check Email: $ python ./AnomDetect-1.py email\n",
    "\n",
    "Is it working? Watch it work. $ python -u ./AnomDetect-1.py\n",
    "\n",
    "Deploy. $ nohup python -u ./AnomDetect-1.py >out.txt 2>out.txt &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-karen",
   "metadata": {},
   "outputs": [],
   "source": [
    "lastCodeUpdate = '1-April-2021' # ref: what-has-changed.txt\n",
    "\n",
    "# What has changed?\n",
    "#   There are command line checks for either 'db' or 'email'\n",
    "#   Detects changes in DB credentials and then uses the credentials... without stoping the AD system from running.\n",
    "#   If unable to connect or query DB, then refreshes DB credentials, sleeps, then continues main loop\n",
    "#   data_max_samples_modeled is working. The raw collected data is never archieved or removed. \n",
    "#     the X most recent rows are used in the model. This means, you can reduce the number of rows modeled (up or down) as desired.\n",
    "#   There are now two models available, kmeans and Isoluation Forest. They are very different, though the plot can look very similar, because it is... only the score changes, not the point.\n",
    "#   You can change the size of the charts and the point size.\n",
    "#   The charts now show all anomalous points in magenta with the current sample in green or red.\n",
    "#     This allows you to see where and what the model considers anomalous... great for adjusting and learning.\n",
    "#   You can change the orientationof the 3D chart.\n",
    "#   You can set the dimentional reduction model (PCA, ICA) in the configuration file.\n",
    "#   All configuration file changes take affect just before the next sample. So, you can experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-chorus",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Testing    = False\n",
    "InNotebook = True\n",
    "CmdCheck   = '' # Only used if InNotebook, then normally, set to '' or to do a check, set to either 'db' or 'email'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-tribe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Directory Settings\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if not InNotebook:\n",
    "    try:\n",
    "        baseDir    = str(sys.argv[1]) + '/'\n",
    "        configFile = str(sys.argv[2])\n",
    "    except:\n",
    "        baseDir    = os.getcwd() + '/'\n",
    "        configFile = 'AnomDetect.cfg'\n",
    "else:\n",
    "    baseDir    = os.getcwd() + '/'\n",
    "    configFile = 'AnomDetect.cfg'\n",
    "\n",
    "chartsDir2D = baseDir + 'charts2D'   # <-------- Make sure this directory exists!!!\n",
    "chartsDir3D = baseDir + 'charts3D'   # <-------- Make sure this directory exists!!!\n",
    "alertFN     = baseDir + 'alertlog.txt'\n",
    "\n",
    "print()\n",
    "print('Directories and files:')\n",
    "print('  baseDir    ', baseDir)\n",
    "print('  chartsDir2D', chartsDir2D)\n",
    "print('  chartsDir3D', chartsDir3D)\n",
    "print('  configFile ', configFile)\n",
    "print('  alertFN    ', alertFN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doExit():\n",
    "    import sys\n",
    "    print('Exiting clean')\n",
    "    try:    \n",
    "        cursor.close()\n",
    "        dbConnection.close()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    sys.exit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-fetish",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alertLogWrite(line_in):\n",
    "\n",
    "    from datetime import datetime\n",
    "\n",
    "    now = datetime.now().strftime(\"%d-%b-%Y %H:%M:%S\")\n",
    "\n",
    "    f = open(alertFN,'a')\n",
    "    f.write(now + ', ' + line_in + '\\n')\n",
    "    \n",
    "if Testing:\n",
    "    x = 'Yo Craig, This is an alert log test.'\n",
    "    alertLogWrite(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkDBFileChange():\n",
    "\n",
    "    # Check if DB access credentials have changed in the AnomDetect_DB.py file.\n",
    "    \n",
    "    change = False\n",
    "    \n",
    "    import AnomDetect_DB as db\n",
    "    import importlib\n",
    "    \n",
    "    current_user = db.user\n",
    "    current_pw   = db.pw\n",
    "    current_dsn  = db.dsn\n",
    "    \n",
    "    importlib.reload(db)\n",
    "    \n",
    "    if (current_user != db.user or current_pw != db.pw or current_dsn != db.dsn):\n",
    "        change = True\n",
    "    else:\n",
    "        change = False\n",
    "    \n",
    "    return(change)\n",
    "\n",
    "if Testing:\n",
    "    print('Testing Function: checkDBFileChange')\n",
    "    print(checkDBFileChange())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-compromise",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def readParamFile(verbose_in):\n",
    "    \n",
    "    # ref: https://zetcode.com/python/configparser\n",
    "    \n",
    "    import configparser\n",
    "    \n",
    "    config = configparser.ConfigParser()\n",
    "    \n",
    "    fullConfigFile = baseDir + configFile\n",
    "    config.read(fullConfigFile)\n",
    "    \n",
    "    try:\n",
    "        prior = parCore\n",
    "    except:\n",
    "        prior = config['core']\n",
    "    \n",
    "    myParCore = config['core']\n",
    "    \n",
    "    pars = str({section: dict(config[section]) for section in config.sections()})\n",
    "    \n",
    "    if verbose_in:\n",
    "        print('\\nParameter Settings:\\n')\n",
    "        print(pars, '\\n')\n",
    "        alertLogWrite('Current Parameters, ' + pars)\n",
    "    \n",
    "    if prior != myParCore:\n",
    "        alertLogWrite('Detected parameter file change, ' + pars)\n",
    "        print('\\nParemeter Setting CHANGE:\\n')\n",
    "        print(pars, '\\n')\n",
    "    \n",
    "    return(myParCore)\n",
    "\n",
    "if Testing:\n",
    "    print('Testing Function: readParamFile')\n",
    "    # pareCore is outside of the function, so results available everywhere\n",
    "    parCore = readParamFile(True) # keep result as parCore, so it can be used when testing\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-least",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendAlertEmail_wo_charts(subject_in, message_in, verbose_in):\n",
    "    \n",
    "    # Ref: https://realpython.com/python-send-email/\n",
    "    \n",
    "    result = False\n",
    "\n",
    "    import AnomDetect_Email as email\n",
    "    import smtplib\n",
    "    import ssl\n",
    "    import importlib\n",
    "    \n",
    "    importlib.reload(email)\n",
    "\n",
    "    port           = email.port\n",
    "    smtp_server    = email.smtp_server\n",
    "    sender_email   = email.sender_email\n",
    "    sender_pass    = email.sender_pass\n",
    "    receiver_email = email.receiver_email\n",
    "\n",
    "    fullMessage = \"Subject: \" + str(subject_in) + \"\\n\\n\" + message_in\n",
    "\n",
    "    try:\n",
    "        context = ssl.create_default_context()\n",
    "        with smtplib.SMTP_SSL(smtp_server, port, context=context) as server:\n",
    "            server.login(sender_email, sender_pass)\n",
    "            server.sendmail(sender_email, receiver_email, fullMessage)\n",
    "        result = True\n",
    "    except:\n",
    "        print('Email alert message failed')\n",
    "\n",
    "    return(result)\n",
    "\n",
    "# To perform the send test, remove both sets of the 3 double quotes below\n",
    "\"\"\"\n",
    "if Testing:\n",
    "    print('Testing Function: sendAlertEmail \\n')\n",
    "    from datetime import datetime\n",
    "    now = datetime.now().strftime(\"%d-%b-%Y %H:%M:%S\")\n",
    "    mySubject = 'AD1 anomaly detected at ' + now\n",
    "    myMessage = 'Hey Craig, This is an email TEST at ' + now\n",
    "    \n",
    "    if str2bool(parCore['alert_email_wo_charts']):\n",
    "        q = sendAlertEmail_wo_charts(mySubject, myMessage, True)\n",
    "        print('q',q)\n",
    "    else:\n",
    "        print('Alert email WO CHARTS is not enabled.')\n",
    "\"\"\"\n",
    "\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-forge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendAlertEmail_w_charts(subject_in, message_in, filename_in, verbose_in):\n",
    "    \n",
    "    # Ref: https://realpython.com/python-send-email/\n",
    "    \n",
    "    # filename_in is NOT the full path, just the image filename\n",
    "    \n",
    "    result = False\n",
    "    \n",
    "    import importlib\n",
    "    import AnomDetect_Email as e\n",
    "    importlib.reload(e)\n",
    "    \n",
    "    import email, smtplib, ssl\n",
    "    \n",
    "    from email import encoders\n",
    "    from email.mime.base import MIMEBase\n",
    "    from email.mime.multipart import MIMEMultipart\n",
    "    from email.mime.text import MIMEText\n",
    "    from email.mime.image import MIMEImage\n",
    "        \n",
    "    # Create a multipart message and set headers\n",
    "    message = MIMEMultipart()\n",
    "    message[\"From\"] = e.sender_email\n",
    "    message[\"To\"] = e.receiver_email\n",
    "    message[\"Subject\"] = subject_in\n",
    "    \n",
    "    # Add body to email\n",
    "    message.attach(MIMEText(message_in, \"plain\"))\n",
    "    \n",
    "    for myDir in [chartsDir2D, chartsDir3D]:\n",
    "        \n",
    "        # Open graphic file in binary mode\n",
    "        with open(myDir + '/' + filename_in, \"rb\") as attachment:\n",
    "            # Add file as application/octet-stream\n",
    "            # Email client can usually download this automatically as attachment\n",
    "            part = MIMEBase(\"application\", \"octet-stream\")\n",
    "            part.set_payload(attachment.read())\n",
    "        \n",
    "        # Encode file in ASCII characters to send by email    \n",
    "        encoders.encode_base64(part)\n",
    "        \n",
    "        # Add header as key/value pair to attachment part\n",
    "        part.add_header(\n",
    "            \"Content-Disposition\",\n",
    "            f\"attachment; filename= {filename_in}\",\n",
    "        )\n",
    "        \n",
    "        # Add attachment to message and convert message to string\n",
    "        message.attach(part)\n",
    "        text = message.as_string()\n",
    "    \n",
    "    # Log in to server using secure context and send email\n",
    "    context = ssl.create_default_context()\n",
    "    with smtplib.SMTP_SSL(e.smtp_server, e.port, context=context) as server:\n",
    "        server.login(e.sender_email, e.sender_pass)\n",
    "        server.sendmail(e.sender_email, e.receiver_email, text)\n",
    "    result = True\n",
    "    \n",
    "    return(result)\n",
    "\n",
    "# To perform the send test, remove both sets of the 3 double quotes below\n",
    "\"\"\"\n",
    "if Testing:\n",
    "    print('Testing Function: sendAlertEmail WITH CHARTS \\n')\n",
    "    from datetime import datetime\n",
    "    now = datetime.now().strftime(\"%d-%b-%Y %H:%M:%S\")\n",
    "    mySubject = 'AD1 anomaly detected at ' + now\n",
    "    myMessage = 'Hey Craig, This is an email TEST at ' + now + ' with CHARTS!'\n",
    "    \n",
    "    try:\n",
    "        if str2bool(parCore['alert_email_w_charts']):\n",
    "            myFN = \"20210323072349-False.png\"\n",
    "            q = sendAlertEmail_w_charts(mySubject, myMessage, myFN , True)\n",
    "            print('q',q)\n",
    "        else:\n",
    "            print('Alert email WITH CHARTS is not enabled.')\n",
    "    except:\n",
    "        print('For test to work, you need an existing filename...sorry.')\n",
    "\"\"\"\n",
    "\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-chinese",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def makeDbConnection(show_error_in, verbose_in):\n",
    "    \n",
    "    import AnomDetect_DB as db\n",
    "    import cx_Oracle\n",
    "    import os\n",
    "    import importlib\n",
    "    \n",
    "    importlib.reload(db)\n",
    "    \n",
    "    conTF = False\n",
    "    \n",
    "    if verbose_in:\n",
    "        print('Attempting Oracle connection db.user db.dsn ', db.user, db.dsn, end=' ...')\n",
    "    \n",
    "    try:\n",
    "        conDetails = cx_Oracle.connect(db.user, db.pw, db.dsn)\n",
    "        \n",
    "    except cx_Oracle.Error as e:\n",
    "        conDetails = 0\n",
    "        \n",
    "        if show_error_in:            \n",
    "            errorObj, = e.args\n",
    "            print('failed to connect.')\n",
    "            print('   Error Code       :', errorObj.code)\n",
    "            print('   Error Message    :', errorObj.message)\n",
    "            print('   TNS_ADMIN        :', os.getenv('TNS_ADMIN'))\n",
    "            print('   PATH             :', os.getenv('PATH'))\n",
    "            print('   DYLD_LIBRARY_PATH:', os.getenv('DYLD_LIBRARY_PATH'))\n",
    "    \n",
    "    else:\n",
    "        conTF = True\n",
    "        if verbose_in:\n",
    "            print('success.')\n",
    "            print('Connected Oracle version', conDetails.version)\n",
    "        \n",
    "    return( conTF, conDetails )\n",
    "\n",
    "\n",
    "if Testing:\n",
    "    print('Testing Function: makeDbConnection \\n')\n",
    "    \n",
    "    print('Test 1')\n",
    "    result, dbConnection = makeDbConnection(show_error_in=True,verbose_in=True)\n",
    "    print('Test 1 result', result, 'dbConnection', dbConnection)\n",
    "    \n",
    "    print('\\nTest 2')\n",
    "    result, dbConnection = makeDbConnection(show_error_in=True,verbose_in=False)\n",
    "    print('Test 2 result', result, 'dbConnection', dbConnection)\n",
    " \n",
    "    print('\\nDone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-davis",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def querySourceCheck(dbConnection_in, verbose_in):\n",
    "    \n",
    "    import cx_Oracle\n",
    "    \n",
    "    myResult = False\n",
    "    \n",
    "    if verbose_in:\n",
    "        print('Attempting to query from data source')\n",
    "\n",
    "    try:\n",
    "        cursor = dbConnection_in.cursor()\n",
    "    \n",
    "    except cx_Oracle.Error as e:\n",
    "        errorObj, = e.args\n",
    "        print('failed to query.')\n",
    "        print('   Error Code       :', errorObj.code)\n",
    "        print('   Error Message    :', errorObj.message)\n",
    "\n",
    "    else:\n",
    "        sql = \"select distinct(metric_name) from \" + parCore['oracle_perf_data_table_name'] \\\n",
    "              + \" \" + parCore['oracle_perf_data_where_clause'] + \" order by metric_name\"\n",
    "        if verbose_in:\n",
    "            print('sql', sql)\n",
    "        \n",
    "        try:\n",
    "            cursor.execute(sql)\n",
    "            #print( cursor.fetchmany(5) )\n",
    "            if verbose_in:\n",
    "                print( cursor.fetchall())\n",
    "            \n",
    "            myResult = True\n",
    "        \n",
    "        except:\n",
    "            myResult = False\n",
    "            \n",
    "    cursor.close()\n",
    "    \n",
    "    return(myResult)\n",
    "\n",
    "if Testing:\n",
    "    print('Testing Function: querySourceCheck')\n",
    "    \n",
    "    result, conTest = makeDbConnection(show_error_in=True,verbose_in=True)\n",
    "    print(\"\\nTest 1 True\")\n",
    "    print( querySourceCheck(conTest, True) )\n",
    "\n",
    "    print(\"\\nTest 2 False\")\n",
    "    print( querySourceCheck(conTest, False) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-metadata",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def loadData(fn_in, verbose_in):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    resultTF = False\n",
    "    resultDF = pd.DataFrame()\n",
    "    \n",
    "    if verbose_in:\n",
    "        print(\"Loading data \"+ str(fn_in), end=\" ... \")\n",
    "    \n",
    "    try:\n",
    "        resultDF = pd.read_csv(fn_in, low_memory=False)\n",
    "    \n",
    "    except:\n",
    "        if verbose_in:\n",
    "            print('not found.')\n",
    "    \n",
    "    else:\n",
    "        resultTF = True\n",
    "        resultDF.columns = map(str.lower, resultDF.columns) # all features to be in lower case\n",
    "        resultDF.fillna(0) # replace NaN/Nulls with zero\n",
    "        \n",
    "        if verbose_in:\n",
    "            print(\"done. shape\", resultDF.shape)\n",
    "    \n",
    "    return(resultTF, resultDF)\n",
    "\n",
    "if Testing:\n",
    "    print('Testing Function: loadData \\n')\n",
    "    \n",
    "    print('Test 1 - Should succed')\n",
    "    x = 'http://filebank.orapub.com/DataSets/quick_restart-1.csv'\n",
    "    result, xDF = loadData(x, True)\n",
    "    if result:\n",
    "        print('xDF.shape', xDF.shape)\n",
    "    \n",
    "    print('\\nTest 2 - Should fail')\n",
    "    x = 'http://filebank.orapub.COMMM/DataSets/quick_restart-1.csv'\n",
    "    result, xDF = loadData(x, True)\n",
    "    if result:\n",
    "        print('xDF.shape', xDF.shape)\n",
    "    \n",
    "    print('\\nDone.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-constitutional",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def loadRestartData(verbose_in, showTime_in):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import datetime\n",
    "    \n",
    "    import time\n",
    "    t0 = time.perf_counter()\n",
    "    \n",
    "    resultTF          = False\n",
    "    resultDF          = pd.DataFrame()\n",
    "    oldestRowDatetime = 0\n",
    "    newestRowDatetime = 0\n",
    "    oldestDatetime    = 0\n",
    "    newestDatetime    = 0\n",
    "        \n",
    "    if parCore['data_restart_enabled']:\n",
    "        \n",
    "        csvFN  = parCore['data_restart_csv_file']\n",
    "        httpFN = parCore['data_restart_http_base']\n",
    "        \n",
    "        if verbose_in:\n",
    "            print('Loading CSV', csvFN, end=\" ... \")\n",
    "        resultTF, resultDF = loadData(csvFN,verbose_in)\n",
    "        \n",
    "        if not resultTF:\n",
    "            if verbose_in:\n",
    "                print('\\nLoading http', httpFN + '/' + csvFN)\n",
    "            resultTF, resultDF = loadData(httpFN + '/' + csvFN,verbose_in)\n",
    "        \n",
    "        if not resultTF:\n",
    "            print('No restart data (csv, http) found.')\n",
    "        else:\n",
    "            resultDF.columns = map(str.lower, resultDF.columns) # all features to be in lower case\n",
    "            x = resultDF.dt.head(1)\n",
    "            oldest = x.to_string(index=False)\n",
    "            x = resultDF.dt.tail(1)\n",
    "            newest = x.to_string(index=False)\n",
    "            \n",
    "            if verbose_in:\n",
    "                print('oldest', type(oldest), oldest)\n",
    "                print('newest', type(newest), newest)\n",
    "            \n",
    "            oldestDatetime = datetime.datetime.strptime(oldest, '%Y%m%d%H%M%S')\n",
    "            #print('oldestDatetime', oldestDatetime)\n",
    "            newestDatetime = datetime.datetime.strptime(newest, '%Y%m%d%H%M%S')\n",
    "            #print('newestDatetime', newestDatetime)\n",
    "            if verbose_in:\n",
    "                print('oldestDatetime newestDatetime', oldestDatetime, newestDatetime, end='')\n",
    "        \n",
    "        if showTime_in:\n",
    "            print(f'(LR et {time.perf_counter() - t0:0.1f}s)')\n",
    "        else:\n",
    "            print()\n",
    "    else:\n",
    "        if verbose_in:\n",
    "            print('Data restart is not enabled.')\n",
    "\n",
    "    return(resultTF,resultDF, oldestDatetime, newestDatetime)\n",
    "\n",
    "if Testing:\n",
    "    print('Testing Function: loadRestartData')\n",
    "    \n",
    "    gotDataTF, xDF, bDt, eDt = loadRestartData(True, True)\n",
    "    if gotDataTF:\n",
    "        print('\\nDoublecheck')\n",
    "        print('xDF.shape', xDF.shape)\n",
    "        print('bDt, eDt', bDt, eDt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-rolling",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def checkConnectAndQuery(verbose_in):\n",
    "    \n",
    "    result = False\n",
    "\n",
    "    conResult, mydbcon = makeDbConnection(show_error_in=True,verbose_in=verbose_in)\n",
    "    \n",
    "    if conResult:\n",
    "        result  = querySourceCheck(mydbcon, verbose_in)\n",
    "    \n",
    "    return(result)\n",
    "\n",
    "if Testing:\n",
    "    print('Testing Function: checkConnectAndQuery \\n')\n",
    "\n",
    "    print('Test 1 - True verbose, should return True')\n",
    "    print( checkConnectAndQuery(True) )\n",
    "    \n",
    "    print('\\nTest 2 - False verbose, should return True')\n",
    "    print( checkConnectAndQuery(False) )\n",
    "    \n",
    "    print('\\nDone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-insider",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getMaxDate(dataType_in, dataSource_in, verbose_in):\n",
    "    \n",
    "    # Only the date is returned, not the metrics\n",
    "    \n",
    "    if dataType_in == 'dataframe':\n",
    "        if len(dataSource_in) > 0:\n",
    "            # if the below astype is not used, an error returned related to types\n",
    "            maxDate = dataSource_in['dt'].astype('int64').max()\n",
    "        else:\n",
    "            maxDate = 0\n",
    "    \n",
    "    elif dataType_in == 'oracle':\n",
    "        try:\n",
    "            # dataSource_in is not used\n",
    "            sql = \"select max(to_number(to_char(begin_time, 'YYYYMMDDHH24MISS'))) dt from \" \\\n",
    "                  + parCore['oracle_perf_data_table_name'] + \" \" \\\n",
    "                  + parCore['oracle_perf_data_where_clause']\n",
    "\n",
    "            cursor = dbConnection.cursor()\n",
    "            cursor.execute(sql)\n",
    "            row = cursor.fetchone()\n",
    "            cursor.close()\n",
    "            maxDate = row[0]\n",
    "        except:\n",
    "            print('Unable to perform DB query. Function getMaxDate.')\n",
    "            maxDate = 0\n",
    "    \n",
    "    else:\n",
    "        if verbose_in:\n",
    "            print('getMaxDate: Invalid dataType_in, returning 0')\n",
    "        maxDate = 0\n",
    "    \n",
    "    if verbose_in:\n",
    "        print('getMaxDate return:', maxDate)\n",
    "        \n",
    "    return(maxDate)\n",
    "    \n",
    "if Testing:\n",
    "    print('Testing Function: getMaxDate')\n",
    "    \n",
    "    import pandas\n",
    "    \n",
    "    print('Test 1 - Should return data. Test: normal ora')\n",
    "    print( getMaxDate('oracle','na', True) )\n",
    "    \n",
    "    print('\\nTest 2 - Should NOT return data. Test: empty DF')\n",
    "    myCols = ['name', 'dt']\n",
    "    df = pandas.DataFrame(columns = myCols)\n",
    "    print( getMaxDate('dataframe',df, True) )\n",
    "    \n",
    "    print('\\nTest 3 - Should return data. Test: normal DF')\n",
    "    data = [['tom', 10], ['nick', 15], ['juli', 14]] \n",
    "    df = pandas.DataFrame(data, columns = ['name', 'dt'])\n",
    "    print( getMaxDate('dataframe',df, True) )\n",
    "    \n",
    "    print('\\nTest 4 - Should NOT return data. Test: bogus data type param')\n",
    "    df = pandas.DataFrame(columns = ['name', 'dt'])\n",
    "    print( getMaxDate('woops',df, True) )\n",
    "    \n",
    "    print('\\nTest 5 - Should NOT return data. Test: empty DF')    \n",
    "    df = pandas.DataFrame(columns=myCols)\n",
    "    currentMaxDate = getMaxDate('dataframe', df, True)\n",
    "    print('currentMaxDate', currentMaxDate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-english",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getNewData(lastDataBeginTime_in, verbose_in):\n",
    "    \n",
    "    # if zero rows returned, then no new data\n",
    "    \n",
    "    sql = \"select to_char(begin_time, 'YYYYMMDDHH24MISS') dt, metric_name, value metric_value \" + \\\n",
    "          \"from   \" + parCore['oracle_perf_data_table_name'] + \" \" + \\\n",
    "           parCore['oracle_perf_data_where_clause'] +  \\\n",
    "               \" and  to_char(begin_time, 'YYYYMMDDHH24MISS') > :begTime\"\n",
    "    \n",
    "    lastDataBeginTime = lastDataBeginTime_in\n",
    "    \n",
    "    try:\n",
    "        cursor = dbConnection.cursor()\n",
    "        cursor.execute(sql, begTime = str(lastDataBeginTime_in))  # setting the bind variable\n",
    "        newRows = cursor.fetchall()\n",
    "        cursor.close()\n",
    "        #print('len(newRows)', len(newRows))\n",
    "    except:\n",
    "        newRows = []\n",
    "        print('Unable to query new data. Function, getNewData')\n",
    "    \n",
    "    # if zero rows returned, then no new data\n",
    "    if len(newRows) == 0:\n",
    "        lastDataBeginTime = 0\n",
    "    else:\n",
    "        lastDataBeginTime = newRows[0][0]\n",
    "    \n",
    "    return(lastDataBeginTime, newRows)\n",
    "\n",
    "\n",
    "if Testing:\n",
    "    print('Testing Function: getNewData')\n",
    "    \n",
    "    print('Test 1 - should return data')\n",
    "    dt, rows = getNewData('198901010101', True)\n",
    "    print(dt, rows)\n",
    "\n",
    "    print('\\nTest 2 - should not return data')\n",
    "    print(getNewData(dt, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-carry",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def resetCoreDFtypes(DF_in, verbose_in):\n",
    "    \n",
    "    #if verbose_in:\n",
    "    #    print('In:')\n",
    "    #    print(DF_in.dtypes)\n",
    "\n",
    "    DF_in['dt']           = DF_in['dt'].astype('int64')\n",
    "    DF_in['metric_name']  = DF_in['metric_name'].astype('str')\n",
    "    DF_in['metric_value'] = DF_in['metric_value'].astype('float')\n",
    "    \n",
    "    #if verbose_in:\n",
    "    #    print('Out:')\n",
    "    #    print(DF_in.dtypes)\n",
    "    \n",
    "    return(DF_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addNewRows(coreDF_in, verbose_in):\n",
    "        \n",
    "    result = False\n",
    "    availableMaxDate = 0\n",
    "    \n",
    "    try:\n",
    "        #print('A')\n",
    "        # Get most recent date for the data collected and data available\n",
    "        currentMaxDate    = getMaxDate('dataframe', coreDF_in, verbose_in)\n",
    "        #print('B')\n",
    "        \n",
    "        availableMaxDate  = getMaxDate('oracle','na', verbose_in)\n",
    "        \n",
    "        # If new is available, get it\n",
    "        #print('C')\n",
    "        \n",
    "        if int(availableMaxDate) > int(currentMaxDate):\n",
    "            #print('D')\n",
    "            \n",
    "            newMaxDate, newRows = getNewData(currentMaxDate, verbose_in)\n",
    "            newRowsDF           = pd.DataFrame(newRows, columns=coreDF_in.columns)\n",
    "            #print('E')\n",
    "        \n",
    "        else:\n",
    "            newRowsDF           = pd.DataFrame(columns=coreDF_in.columns)\n",
    "            #print('F')\n",
    "        \n",
    "        # Set data types\n",
    "        coreDF_in = resetCoreDFtypes(coreDF_in, verbose_in)\n",
    "        #print('G')\n",
    "        \n",
    "        # Add new data to existing data\n",
    "        appendedDF = coreDF_in.append(newRowsDF)\n",
    "        #print('H')\n",
    "        \n",
    "        # Always print this... it's like a heartbeat\n",
    "        #\n",
    "        print('rawDF', coreDF_in.shape, 'newRowsDF', newRowsDF.shape, 'appendedDF', appendedDF.shape, end='')\n",
    "\n",
    "        if verbose_in:\n",
    "            #print(type(appendedDF))\n",
    "\n",
    "            # https://www.geeksforgeeks.org/selecting-rows-in-pandas-dataframe-based-on-conditions/\n",
    "            x = appendedDF[appendedDF['metric_name'] == 'Average Active Sessions'].metric_value.tail(1)\n",
    "            print(' AAS', x.to_string(index=False))\n",
    "        \n",
    "        result = True\n",
    "    except Exception as e:\n",
    "        print('\\nException. Function: addNewRows:',e)\n",
    "        appendedDF = coreDF_in.copy()\n",
    "        print('\\nUnable to retreive new data. Function, addNewRows 1\\n')\n",
    "    \n",
    "    if availableMaxDate == 0:\n",
    "        result = False\n",
    "        appendedDF = coreDF_in.copy()\n",
    "        print('\\nUnable to retreive new data. Function, addNewRows 2\\n')\n",
    "        \n",
    "    # Return the combined existing and new data\n",
    "    return(result, appendedDF)\n",
    "\n",
    "if Testing:\n",
    "    print('Testing Function: addNewRows')\n",
    "    \n",
    "    mySleepSec = 0\n",
    "    \n",
    "    import pandas as pd\n",
    "    myCols=['dt','metric_name','metric_value']\n",
    "    qDF = pd.DataFrame(columns=myCols)\n",
    "    \n",
    "    print('Test 1 - Should return data but no Verbose\\n')\n",
    "    qResult, qDF = addNewRows(qDF, False)\n",
    "    print('qDF.shape', qDF.shape)\n",
    "    print(' qDF[dt].head(10)')\n",
    "    print(qDF['dt'].head(10))\n",
    "    \n",
    "    print('\\nTest 2 - Should NOT return data\\n')\n",
    "    print('   Calling addNewRows...')\n",
    "    qResult, qDF = addNewRows(qDF, True)\n",
    "    \n",
    "    print('\\nTest 3 - Sleeping for', mySleepSec, 'seconds...')\n",
    "    import time\n",
    "    time.sleep(mySleepSec)\n",
    "    print('\\n         Should return data if enough sleep time.\\n')\n",
    "    qResult, qDF = addNewRows(qDF, True)\n",
    "    print('\\nDone.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-spoke",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def checkpoint(lastCheckpoint_in, coreDF_in, verbose_in):\n",
    "    \n",
    "    from datetime import datetime\n",
    "    import pandas as pd\n",
    "        \n",
    "    theLastCheckpoint = lastCheckpoint_in\n",
    "    \n",
    "    diff     = datetime.now() - lastCheckpoint_in\n",
    "    diff_sec = int(diff.days*24*60*60 + diff.seconds)\n",
    "    \n",
    "    if diff_sec > int(parCore['data_restart_checkpoint_s']):\n",
    "        \n",
    "        import time\n",
    "        t0 = time.perf_counter()\n",
    "        \n",
    "        fn = baseDir + parCore['data_restart_csv_file']\n",
    "        \n",
    "        print('\\nCheckpoint begin', end='...')\n",
    "        alertLogWrite('Checkpoint begin, ' + str(fn))\n",
    "        \n",
    "        try:\n",
    "            coreDF_in.to_csv (fn, index = False, header=True)\n",
    "        except:\n",
    "            line = ' Checkpoint, unable to checkpoint.\\n'\n",
    "            print(line)\n",
    "            alertLogWrite(line)\n",
    "        else:\n",
    "            theLastCheckpoint = datetime.now()\n",
    "\n",
    "            rows = str(coreDF_in.shape[0])\n",
    "            et   = str(round(time.perf_counter() - t0,1))\n",
    "            print('complete.', rows, et + 's', end='')\n",
    "            alertLogWrite('Checkpoint end, ' + rows + ' rows, ' + et + ' sec')\n",
    "    \n",
    "    else:\n",
    "        if verbose_in:\n",
    "            print('Checkpoint occured', diff_sec, 'seconds ago.')\n",
    "    \n",
    "    return(theLastCheckpoint)\n",
    "\n",
    "\"\"\"\n",
    "# This test will RESET the checkpoint file. That is, DESTROY the existing checkpoint file\n",
    "\n",
    "print('Test 1 - Checkpoint should occur')\n",
    "from datetime import datetime, timedelta\n",
    "whileBack = datetime.now() + timedelta(days=-1)\n",
    "last = checkpoint(whileBack, coreDF, True)\n",
    "\n",
    "print('\\nTest 2 - Checkpoint should NOT occur')\n",
    "checkpoint(last, coreDF, True)    \n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-annex",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def defineCoreDF(verbose_in):\n",
    "    try:\n",
    "        del coreDF\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    myDF = pd.DataFrame(columns=['dt','metric_name', 'metric_value'])\n",
    "    myDF = resetCoreDFtypes(myDF, verbose_in)\n",
    "    \n",
    "    return(myDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-makeup",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Denormalize for sysmetric type data, with columns dt (yyyymmddhhss), metric_name, metric_value\n",
    "\n",
    "def denormalize(df_in,verbose_in):\n",
    "    \n",
    "    if verbose_in:\n",
    "        print(\"Denormalizing\")\n",
    "        print(\"   BEFORE \", df_in.shape)\n",
    "    \n",
    "    # The \"values\" will become the new \"columns\" value\n",
    "    df_inPiv = df_in.pivot_table(index='dt', values='metric_value', columns=['metric_name'])\n",
    "    df_inPiv.reset_index(inplace=True)\n",
    "\n",
    "    if verbose_in:\n",
    "        print(\"   AFTER  \", df_inPiv.shape)\n",
    "        print(\"done.\")\n",
    "\n",
    "    return(df_inPiv)\n",
    "    \n",
    "    \n",
    "if Testing:\n",
    "    print(\"Testing Function: denormalize \\n\")\n",
    "    \n",
    "    x = 'ad_test_data_1b.csv'\n",
    "    x = 'quick_restart-fun.csv'\n",
    "    result, testDF = loadData(x, True)\n",
    "    if not result:\n",
    "        x = 'http://filebank.orapub.com/DataSets/ad_test_data_1b.csv'\n",
    "        result, testDF = loadData(x, True)\n",
    "    \n",
    "    print('\\nBEFORE testDF.shape  ', testDF.shape)\n",
    "    if result:\n",
    "        print('testDF.columns.list', testDF.columns)\n",
    "        #features = ['dt','metric_name','metric_value']\n",
    "        #testDF = testDF[features]\n",
    "        testDFpiv = denormalize(testDF, True)\n",
    "        print('\\nAFTER testDFpiv.shape', testDFpiv.shape)\n",
    "        print('testDFpiv.columns.list', testDFpiv.columns)\n",
    "        print('\\ntestDF[\"dt\"].head()')\n",
    "        print(testDF['dt'].head())\n",
    "\n",
    "    print('\\nDone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-gross",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def EngineerFeatures(DF_in, verbose_in):\n",
    "    \n",
    "    # Expecting DF_in to be denormalized from some v$sysmetric type view\n",
    "    # Each new feature should have a corresponding T/F entry in the configuration file\n",
    "    # Using pd.get_dummies to one-hot-encode the new feature\n",
    "    \n",
    "    import datetime\n",
    "    import pandas as pd\n",
    "    \n",
    "    if verbose_in:\n",
    "        print('Engineering features', end=' ... ')\n",
    "    \n",
    "    myWorkDF       = DF_in.copy()\n",
    "    myWorkDF['dt'] = myWorkDF['dt'].astype('int64') \n",
    "    \n",
    "    myWorkDF['datetime'] = pd.to_datetime(myWorkDF['dt'], format='%Y%m%d%H%M%S')\n",
    "    \n",
    "    # Hour Of Day - feature_enable_hourofday\n",
    "    \n",
    "    if str2bool(parCore['feature_enable_hourofday']):\n",
    "        if verbose_in:\n",
    "            print('hourofday', end=' ... ')\n",
    "        myWorkDF['hourofday']   = myWorkDF['datetime'].dt.hour\n",
    "        myWorkDF.fillna({'hourofday':0}, inplace = True)\n",
    "        myWorkDF = pd.concat([myWorkDF, pd.get_dummies(myWorkDF['hourofday'], prefix='hod', dummy_na=True)], axis=1)\n",
    "        myWorkDF = myWorkDF.drop(columns=['hourofday'])\n",
    "    \n",
    "    # Day Of Week - feature_enable_dayofweek\n",
    "    \n",
    "    if str2bool(parCore['feature_enable_dayofweek']):\n",
    "        if verbose_in:\n",
    "            print('dayofweek', end=' ... ')\n",
    "        myWorkDF['dayofweek'] = myWorkDF['datetime'].dt.dayofweek\n",
    "        myWorkDF.fillna({'dayofweek':0}, inplace = True)\n",
    "        myWorkDF = pd.concat([myWorkDF, pd.get_dummies(myWorkDF['dayofweek'], prefix='dow', dummy_na=True)], axis=1)\n",
    "        myWorkDF = myWorkDF.drop(columns=['dayofweek'])\n",
    "    \n",
    "    # Week Of Month - feature_enable_weekofmonth\n",
    "    \n",
    "    if str2bool(parCore['feature_enable_weekofmonth']):\n",
    "        if verbose_in:\n",
    "            print('weekofmonth', end=' ... ')\n",
    "        myWorkDF['weekofmonth'] = myWorkDF['datetime'].apply(lambda d: (d.day-1) // 7 + 1)\n",
    "        myWorkDF.fillna({'weekofmonth':0}, inplace = True)\n",
    "        myWorkDF = pd.concat([myWorkDF, pd.get_dummies(myWorkDF['weekofmonth'], prefix='wom', dummy_na=True)], axis=1)\n",
    "        myWorkDF = myWorkDF.drop(columns=['weekofmonth'])\n",
    "    \n",
    "    myWorkDF = myWorkDF.drop(columns=['datetime'])\n",
    "    \n",
    "    #\n",
    "    # Drop unwanted features here... Feature removal\n",
    "    #\n",
    "    \n",
    "    if verbose_in:\n",
    "        print(myWorkDF.columns)\n",
    "        print(\"done.\")\n",
    "    \n",
    "    return (myWorkDF)\n",
    "\n",
    "if Testing:\n",
    "    print('Testing Function: EngineerFeatures')\n",
    "    parCore = readParamFile(True) # keep result as parCore, so it can be used when testing\n",
    "    print('     BEFORE', testDFpiv.shape)\n",
    "    testDFpiv2 = EngineerFeatures(testDFpiv, True)\n",
    "    print('     AFTER ', testDFpiv2.shape)\n",
    "    print()\n",
    "    print(testDFpiv2.columns)\n",
    "    try:\n",
    "        colsToShow = ['dt', 'wom_3.0','dow_2.0', 'hod_21.0']\n",
    "        print(testDFpiv2[colsToShow].tail(10))\n",
    "        print(testDFpiv2[colsToShow].head(10))\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-pepper",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function: Dimension Reduction, using either:\n",
    "#           PCA: Principle Component Analysis\n",
    "#           ICA: Independent Component Analysis\n",
    "\n",
    "def DimReduce(df_in, model_in, dimensions_in, verbose_in):\n",
    "    \n",
    "    if verbose_in:\n",
    "        print('Reducing dimensionality using', str(model_in), \\\n",
    "              'from', str(df_in.shape), 'to', end=\"... \")\n",
    "    \n",
    "    in_shape = df_in.shape\n",
    "    \n",
    "    if model_in == 'PCA':\n",
    "        \n",
    "        from sklearn.decomposition import PCA                 # load library\n",
    "        pca       = PCA(n_components=dimensions_in)           # init model\n",
    "        array_out = pca.fit_transform(df_in)                  # fit DF_in and transform DF_in\n",
    "        \n",
    "    elif model_in == 'ICA':\n",
    "        \n",
    "        from sklearn.decomposition import FastICA                        # load library\n",
    "        ICA       = FastICA(n_components=dimensions_in, random_state=12) # init model\n",
    "        array_out = ICA.fit_transform(df_in)                             # fit DF_in and transform DF_in\n",
    "        \n",
    "    else:\n",
    "        print('   ERROR Function DimReduce. Invalid model provided.')\n",
    "    \n",
    "    df_out = pd.DataFrame(data = array_out) # create DF from array\n",
    "    \n",
    "    if verbose_in:\n",
    "        print(str(df_out.shape), 'done.')\n",
    "        \n",
    "    return(df_out, array_out)\n",
    "\n",
    "if Testing:\n",
    "    print('Testing Function: DimRed2')\n",
    "    print('     BEFORE', testDFpiv2.shape)\n",
    "    testDFpiv3, bogus = DimReduce(testDFpiv2.drop(columns=['dt']), 'PCA', 2, True)\n",
    "    print('     AFTER ', testDFpiv3.shape)\n",
    "    print()\n",
    "    print(testDFpiv3.head(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Scale\n",
    "#             Standardize: mean=0 stdev=1\n",
    "#             Normanize  : min =  max  =1\n",
    "\n",
    "def Scale(df_in, type_in, verbose_in):\n",
    "    \n",
    "    if verbose_in:\n",
    "        print('Scaling', end='... ')\n",
    "    \n",
    "    if type_in == 'standardize':\n",
    "        if verbose_in:\n",
    "            print('standardizing (mean=0 stdev=1)', end='... ')\n",
    "        \n",
    "        from sklearn.preprocessing import StandardScaler      # load lib\n",
    "        myModel = StandardScaler(with_mean=True, with_std=True).fit(df_in) # init and fit model \n",
    "        myAR    = myModel.transform(df_in)                    # scale/transform df_in, result is an array\n",
    "        myDF    = pd.DataFrame(myAR, columns=df_in.columns)   # convert result array to DF\n",
    "    \n",
    "    elif type_in == 'normalize':\n",
    "        if verbose_in:\n",
    "            print('normalizing (min=0 max=1)', end='... ')\n",
    "        \n",
    "        from sklearn.preprocessing import MinMaxScaler      # load lib\n",
    "        myModel = MinMaxScaler().fit(df_in)                 # init and fit model \n",
    "        myAR    = myModel.transform(df_in)                  # transform/normalize df_in, resuilt is an array\n",
    "        myDF    = pd.DataFrame(myAR, columns=df_in.columns) # convert result array to DF\n",
    "    else:\n",
    "        print('Error in Scale function. type_in=', type_in)\n",
    "        myDF = df_in\n",
    "        myAR = df_in.to_numpy()\n",
    "    \n",
    "    if verbose_in:\n",
    "        print(\"done.\")\n",
    "        \n",
    "    return(myDF)                                            # return the DF, not the Array\n",
    "\n",
    "\n",
    "if Testing:\n",
    "    print('Testing Function: Scale\\n')\n",
    "    print('Raw Data...\\n')\n",
    "    print(testDFpiv2[['Average Active Sessions','Database CPU Time Ratio']].describe())\n",
    "    print()\n",
    "\n",
    "    qaDF = Scale(testDFpiv2.drop(columns=['dt']), 'standardize', True)\n",
    "    print()\n",
    "    print(qaDF[['Average Active Sessions','Database CPU Time Ratio']].describe())\n",
    "    \n",
    "    print()\n",
    "    qaDF = Scale(testDFpiv2.drop(columns=['dt']), 'normalize', True)\n",
    "    print()\n",
    "    print(qaDF[['Average Active Sessions','Database CPU Time Ratio']].describe())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-resident",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Function: From a dataframe, create cluster,\n",
    "# returning the initialized model, fitted model and fitted predict model\n",
    "\n",
    "def create_cluster(df_in, cluster_type_in, cluster_no_in, verbose_in):\n",
    "    \n",
    "    if verbose_in:\n",
    "        print('Creating', cluster_type_in, 'with', str(cluster_no_in), 'clusters', end='... ')\n",
    "    \n",
    "    if cluster_type_in == 'kmeans':\n",
    "        \n",
    "        from sklearn.cluster import KMeans\n",
    "        mymodelinit      = KMeans(n_clusters=cluster_no_in, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "        mymodelfit       = mymodelinit.fit(df_in)\n",
    "        mymodelfitlabels = mymodelfit.labels_\n",
    "        mymodelfitpred   = mymodelfit.predict(df_in)\n",
    "        \n",
    "    elif cluster_type_in == 'MiniBatchKMeans':\n",
    "        \n",
    "        from sklearn.cluster import MiniBatchKMeans\n",
    "        mymodelinit      = MiniBatchKMeans(n_clusters=cluster_no_in, batch_size=100, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "        mymodelfit       = mymodelinit.fit(df_in)\n",
    "        mymodelfitlabels = mymodelfit.labels_\n",
    "        mymodelfitpred   = mymodelfit.predict(df_in)\n",
    "    \n",
    "    elif cluster_type_in == 'IsolationForest':\n",
    "        \n",
    "        my_n_estimators = int(parCore['model_strategy_p1'])\n",
    "        my_n_jobs       = int(parCore['model_strategy_p2'])\n",
    "        \n",
    "        from sklearn.ensemble import IsolationForest\n",
    "        mymodelinit      = IsolationForest(n_estimators=my_n_estimators, max_samples=\"auto\", \\\n",
    "                                           contamination=\"auto\", random_state=1, n_jobs=my_n_jobs, verbose=0)\n",
    "        mymodelfit       = mymodelinit.fit(df_in)\n",
    "        mymodelfitpred   = mymodelfit.predict(df_in)\n",
    "    \n",
    "    else:\n",
    "        print('ERROR in function, create_cluster')\n",
    "        \n",
    "    if verbose_in:\n",
    "        print('done.')\n",
    "    \n",
    "    return(mymodelinit, mymodelfit, mymodelfitpred)\n",
    "\n",
    "\n",
    "if Testing:\n",
    "    \n",
    "    print('Testing Function: create_cluster')\n",
    "    \n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    parCore = readParamFile(True) # keep result as parCore, so it can be used when testing\n",
    "    \n",
    "    from collections import Counter\n",
    "    from numpy import unique\n",
    "    \n",
    "    print('Testing Isolation Forest...\\n')\n",
    "    \n",
    "    fn = 'quick_restart-fun.csv'\n",
    "    result, testDF = loadData(x, True)\n",
    "    if not result:\n",
    "        result, testDF = loadData(parCore['data_restart_http_base'] + '/' + fn, True)\n",
    "        testDF.to_csv (fn, index = False, header=True)\n",
    "        print('Saved', fn)\n",
    "    \n",
    "    ModelScaling = 'standardize' # try: standardize, normalize\n",
    "    testDFpiv = denormalize(testDF, True)\n",
    "    qaDF = Scale(testDFpiv.drop(columns=['dt']), ModelScaling, True)\n",
    "    print()\n",
    "    print(qaDF[['Average Active Sessions','Database CPU Time Ratio']].describe())\n",
    "    \n",
    "    plotPointsDF, plotPointsAR = DimReduce(qaDF, 'PCA', 2, True) # try: PCA and ICA. Only affects plotting\n",
    "    \n",
    "    print('\\nShould see the DimReduce result (scaling has already taken place)')\n",
    "    print(plotPointsDF.describe())\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.title('Isolation Forest - Raw Points After Preprocessing\\n' + 'Scaling Into Model & Plot: ' + str(ModelScaling) )\n",
    "    plt.scatter(plotPointsAR[:,0], plotPointsAR[:,1], s=1, c='blue')\n",
    "    plt.show()\n",
    "    \n",
    "    myclustertype = 'IsolationForest'\n",
    "    myclusterno   = -1\n",
    "    myModelInit, myModelFit, myModelFitPred = create_cluster(qaDF, myclustertype, myclusterno, True)\n",
    "    print('plotPointsDF, qaDF, myModelFitPred', plotPointsDF.shape, qaDF.shape, myModelFitPred.shape, type(myModelFitPred))\n",
    "    print('\\nDescribing Prediction Results (1 or -1)')\n",
    "    print(pd.DataFrame(myModelFitPred).describe())\n",
    "    print(Counter(myModelFitPred))\n",
    "    print('\\nScore Samples:\\n')\n",
    "    print(myModelFit.score_samples(qaDF)) # qaDF has be pivited and scaled, but not Dim Reduced\n",
    "    print('---------------')\n",
    "    print(myModelFitPred[:])\n",
    "    \n",
    "    # notice i'm loading the plotPointsDF with fitted, scores from qaDF\n",
    "    # notice i'm multiplying the score by -1, since lower score = more anomalous... i want it reversed\n",
    "    plotPointsDF['score'] = -1*myModelFit.score_samples(qaDF)\n",
    "    print('\\nDescribing Prediction Result SCORES')\n",
    "    print(plotPointsDF['score'].describe())\n",
    "    fig = plt.figure()\n",
    "    plt.hist(plotPointsDF['score'])\n",
    "    plt.show()\n",
    "    \n",
    "    myScoreThresholdPCT = 0.95 # INCREASE number to see LESS anoamlies... set the threshold higher\n",
    "    myScoreThresholdVAL = np.quantile(plotPointsDF['score'], myScoreThresholdPCT)\n",
    "    print('myScoreThresholdPCT, myScoreThresholdVAL', myScoreThresholdPCT, myScoreThresholdVAL)\n",
    "    plotPointsDF['pointColor'] = 'blue'\n",
    "    plotPointsDF.loc[plotPointsDF['score'] > myScoreThresholdVAL, 'pointColor'] = 'red'\n",
    "    print(Counter(plotPointsDF['pointColor']))\n",
    "    print(plotPointsDF.head())\n",
    "    fig = plt.figure()\n",
    "    plt.title('Isolation Forest - Raw Points After Preprocessing & Scoring'  + '\\nScaling Into Model: ' + str(ModelScaling)+ '   Score Threshold:' + str(myScoreThresholdPCT))\n",
    "    plt.scatter(plotPointsDF[0], plotPointsDF[1], s=1, c=plotPointsDF['pointColor'])\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    \n",
    "    modelList = ['kmeans','MiniBatchKMeans']\n",
    "    \n",
    "    for myclusterno in range(1,3):\n",
    "        \n",
    "        for myclustertype in modelList:\n",
    "\n",
    "            print()\n",
    "            qaDF = testDFpiv2.drop(columns=['dt'])\n",
    "            myModelInit, myModelFit, myModelFitPred = create_cluster(qaDF, myclustertype, myclusterno, True)\n",
    "            print(\"\")\n",
    "            print(\"   Cluster type        :\", myclustertype)\n",
    "            print(\"   Cluster numbers     :\", myclusterno)\n",
    "            print(\"   Cluster points      :\", len(myModelFitPred))\n",
    "            print(\"   Counter Fit Labels  :\", Counter(myModelFit.labels_))\n",
    "            print(\"   Counter Fit U Labels:\", unique(myModelFitPred))\n",
    "            print(\"   Cluster model       :\", myModelInit)\n",
    "            print('           center X    :', myModelInit.cluster_centers_[:, 0])\n",
    "            print('           center Y    :', myModelInit.cluster_centers_[:, 1])\n",
    "            print('           center Z    :', myModelInit.cluster_centers_[:, 2])\n",
    "            print('           center Z2   :', myModelInit.cluster_centers_[:, 3])\n",
    "    \n",
    "    print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-doctor",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Used when the model produces a score point to centroid distance is not the strategy\n",
    "# Currently being used with model, Isolation Forest\n",
    "\n",
    "def getModelScore(DF_in, myModel_in, myModelFit_in, verbose_in):\n",
    "    \n",
    "    if verbose_in:\n",
    "        print('In function, getModelScore. Model:', myModel_in)\n",
    "    \n",
    "    DF_in['score']    = -1*myModelFit_in.score_samples(DF_in)\n",
    "    DF_in['distance'] = DF_in['score'] # distance is used for compatility with other clustering methods\n",
    "    \n",
    "    if verbose_in:\n",
    "        print('\\nDescribe')\n",
    "        print(DF_in[['score','distance']].describe())\n",
    "        myScoreThresholdVAL = np.quantile(DF_in['score'], float(parCore['model_threshold_value']))\n",
    "        print('Score Threshold:', parCore['model_threshold_value'], myScoreThresholdVAL)\n",
    "    \n",
    "    return(DF_in)\n",
    "\n",
    "if Testing:\n",
    "    print('Testing function: getModelScore')\n",
    "    \n",
    "    fn = 'quick_restart-fun.csv'\n",
    "    result, testDF = loadData(x, True)\n",
    "    if not result:\n",
    "        result, testDF = loadData(parCore['data_restart_http_base'] + '/' + fn, True)\n",
    "        testDF.to_csv (fn, index = False, header=True)\n",
    "        print('Saved', fn)\n",
    "    \n",
    "    ModelScaling = 'standardize' # try: standardize, normalize\n",
    "    testDFpiv = denormalize(testDF, True)\n",
    "    qaDF = Scale(testDFpiv.drop(columns=['dt']), ModelScaling, True) # try: standardize and normalize\n",
    "    print()\n",
    "    print(qaDF[['Average Active Sessions','Database CPU Time Ratio']].describe())\n",
    "    \n",
    "    myclustertype = 'IsolationForest'\n",
    "    myclusterno   = -1\n",
    "    myModelInit, myModelFit, myModelFitPred = create_cluster(qaDF, myclustertype, myclusterno, True)\n",
    "    qaDF = getModelScore(qaDF, myclustertype, myModelFit, True)\n",
    "    \n",
    "    print('\\ngetModelScore function is complete.\\n')\n",
    "    print(qaDF[['score','distance']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-arctic",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Function: Calculate distances between the a given cluster center and\n",
    "#           every point in the given Dataframe.\n",
    "\n",
    "def getCentroidDistances(cluster_type_in, cluster_init_in, cluster_fit_in, DF_in, cluster_no_in, verbose_in):\n",
    "    \n",
    "    if verbose_in:\n",
    "        print(\"Get_point_to_centroid\")\n",
    "    \n",
    "    # Part 1 - Create a list of distances from the centroid to each point in DF_in\n",
    "    \n",
    "    import numpy as np\n",
    "    from numpy import linalg as LA\n",
    "    \n",
    "    mypoints = DF_in.to_numpy() # convert DF to numpy array\n",
    "    distances=[] # init list\n",
    "    i = 0\n",
    "    \n",
    "    for datapoint in mypoints:\n",
    "        \n",
    "        #print(datapoint)        \n",
    "        distances.append( LA.norm(datapoint - cluster_init_in.cluster_centers_[cluster_no_in]) )        \n",
    "        i = i + 1\n",
    "    \n",
    "    # Part 2 - Add a new feature 'distance' for each point, containing its distance to centroid\n",
    "    \n",
    "    DF_out             = DF_in\n",
    "    DF_out['distance'] = distances  # new feature/column containing their respective point distance\n",
    "    \n",
    "    if verbose_in:\n",
    "        # Calculate statistics\n",
    "        print('      mean=%0.2f median=%0.2f' % ( np.mean(distances), np.median(distances) ))\n",
    "        print('      95-pct=%0.2f 98-pct=%0.2f' % ( np.quantile(distances, 0.95), np.quantile(distances, 0.98) ))\n",
    "        print('      min=%0.2f max=%0.2f' % ( np.min(distances), np.max(distances) ))\n",
    "        print('done.')\n",
    "        \n",
    "    return(DF_out)\n",
    "    \n",
    "\n",
    "if Testing:\n",
    "    print(\"Testing Function: get_point_to_centroid\")\n",
    "    \n",
    "    modelList = ['kmeans','MiniBatchKMeans']\n",
    "    \n",
    "    for myclustertype in modelList:\n",
    "    \n",
    "        print(\"\\n\" + myclustertype + str(\".......................................\\n\"))\n",
    "        \n",
    "        myclusters    = 1 # number of clusters created\n",
    "        myclusterNo   = 0 # cluster number to get point centroid details, starting with 0\n",
    "        qaDF = Scale(testDFpiv2.drop(columns=['dt']), 'standardize', True)\n",
    "        myModel, myModelFit, myCluster = create_cluster(qaDF, myclustertype, myclusters, True)\n",
    "        print()\n",
    "        print(\"      Cluster type   :\", myclustertype)\n",
    "        print(\"      Cluster numbers:\", myclusters)\n",
    "        print(\"      Cluster points :\", len(myCluster))\n",
    "        print(\"      Cluster model  :\", myModel)\n",
    "        print()\n",
    "        \n",
    "        myPointsDF = getCentroidDistances(myclustertype, myModel, myCluster, qaDF, myclusterNo, True)\n",
    "        \n",
    "        print()\n",
    "        print('len(myPointsDF)',len(myPointsDF))\n",
    "        print('myPointsDF.shape', myPointsDF.shape)\n",
    "        print(myPointsDF[['Average Active Sessions','distance']].head(4))\n",
    "\n",
    "    print(\"\\nDone Testing Function: get_point_to_centroid\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-stake",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getChartFN(chartsDir_in, sampleTime_in, anomalyTrue_in):\n",
    "    \n",
    "    return( chartsDir_in + '/' + str(sampleTime_in) + '-' + str(anomalyTrue_in) + '.png' )\n",
    "\n",
    "if Testing:\n",
    "    print( getChartFN('2Ddir', 12345, True) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-footage",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def preprocessCoreDF(DF_in, verbose_in, showTime_in):\n",
    "        \n",
    "    import time\n",
    "    t0 = time.perf_counter()\n",
    "    \n",
    "    if verbose_in:\n",
    "        print('preprocessing now...', end='')\n",
    "    \n",
    "    myClusterNo       = 1\n",
    "    myClusterCenterNo = 0\n",
    "\n",
    "    DFpiv                          = denormalize(DF_in, verbose_in)\n",
    "    DFpivEng                       = EngineerFeatures(DFpiv, verbose_in)\n",
    "    DFpivEngScale                  = Scale(DFpivEng.drop(columns=['dt']), 'standardize', verbose_in)\n",
    "    myModel, myModelFit, myCluster = create_cluster(DFpivEngScale, parCore['model_strategy'], myClusterNo, verbose_in)\n",
    "    if parCore['model_threshold_strategy'] == 'CentroidPercentile':\n",
    "        DFpivEngScaleDist = getCentroidDistances(parCore['model_strategy'], myModel, myCluster, DFpivEngScale, myClusterCenterNo, verbose_in)\n",
    "    elif parCore['model_threshold_strategy'] == 'ScorePercentile':\n",
    "        DFpivEngScaleDist = getModelScore(DFpivEngScale, myModel, myModelFit, verbose_in)\n",
    "    else:\n",
    "        print('ERROR: model_threshold_strategy parameter set to invalid value.')\n",
    "    \n",
    "    if verbose_in:\n",
    "        print()\n",
    "        print('DFpiv, DFpivEngScaleDist', DFpiv.shape, DFpivEngScaleDist.shape )\n",
    "        print()\n",
    "        print(DFpivEngScaleDist[['distance','Average Active Sessions']].describe())\n",
    "        print()\n",
    "        print(DFpivEngScaleDist.columns)\n",
    "       \n",
    "    if showTime_in:\n",
    "        line = f'PP {time.perf_counter() - t0:0.2f}s'\n",
    "        print(' (' + line + ')')\n",
    "        alertLogWrite(line)\n",
    "    else:\n",
    "        print()\n",
    "    \n",
    "    return(DFpiv, DFpivEng, DFpivEngScale, DFpivEngScaleDist)\n",
    "\n",
    "if Testing:\n",
    "    print(\"Testing Function: preprocessCoreDF\")\n",
    "    QhaveRestartData, QrestartDataDF, QrestartOldestDt, QrestartNewestDt = loadRestartData(True, True)\n",
    "\n",
    "    if QhaveRestartData:\n",
    "        print('Restart data loaded from', QrestartOldestDt, 'to', QrestartNewestDt, QrestartDataDF.shape[0], 'rows')\n",
    "        qDF = QrestartDataDF.copy()\n",
    "        qDF = resetCoreDFtypes(qDF, True)\n",
    "        qDFpiv, qDFpivEng, qDFpivEngScale, qDFpivEngScaleDist = preprocessCoreDF(qDF, True, True)\n",
    "        print('Result: qDFpiv, qDFpivEngScaleDist', qDFpiv.shape, qDFpivEngScaleDist.shape )\n",
    "        print('        distance median', qDFpivEngScaleDist['distance'].median())\n",
    "    else:\n",
    "        print('No restart data available, so not testing')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-pollution",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def alertConditionsMet(DF_in, verbose_in):\n",
    "    \n",
    "    myResult = False\n",
    "    \n",
    "    if verbose_in:\n",
    "        print('\\nIn alertConditionsMet function', end=' ... ')\n",
    "    \n",
    "    fullPass = False\n",
    "    check1   = False\n",
    "    check2   = False\n",
    "    check3   = False\n",
    "    check4   = False\n",
    "    \n",
    "    # Check 1. Alerting must be enabled\n",
    "    if str2bool(parCore['alert_enable']):\n",
    "        if verbose_in:\n",
    "            print('alert_enable',parCore['alert_enable'], end=' ... ' )\n",
    "        check1 = True\n",
    "    \n",
    "    # Check 2. Must have enough sample sets to warrent a legitamet anomaly detection check\n",
    "    mySamplesSets = DF_in.dt.nunique() \n",
    "    \n",
    "    if mySamplesSets < int(parCore['alert_min_sample_sets']):\n",
    "        print(' ', mySamplesSets, 'sample sets below threshold of', parCore['alert_min_sample_sets'])\n",
    "    else:\n",
    "        check2 = True\n",
    "    \n",
    "    # Check 3. Must have sufficient gap since previous alert\n",
    "    from datetime import datetime\n",
    "    timeSinceLastAlert = datetime.now() - lastAlert\n",
    "    secsSinceLastAlert = int(timeSinceLastAlert.days*24*60*60 + timeSinceLastAlert.seconds)\n",
    "    if verbose_in:\n",
    "        print(secsSinceLastAlert, 'secs since last alert, need sec gap of', parCore['alert_min_secs_between'])\n",
    "    \n",
    "    if secsSinceLastAlert > int(parCore['alert_min_secs_between']):\n",
    "        check3 = True\n",
    "    \n",
    "    # Check 4. Check if Force Alert enabled\n",
    "    if str2bool(parCore['debug_force_anomaly_enable']):\n",
    "        print('\\ndebug_force_anomaly_enable is TRUE.')\n",
    "        check4 = True\n",
    "    \n",
    "    if (check1 and check2 and check3) or check4 :\n",
    "        fullPass = True\n",
    "    \n",
    "    if fullPass and verbose_in:\n",
    "        print('yes, conditions met with', mySamplesSets, 'sample sets')\n",
    "    elif not fullPass and verbose_in:\n",
    "        print('check1', check1, 'check2', check2, 'check3', check3, 'check4', check4)\n",
    "    \n",
    "    return(fullPass)\n",
    "\n",
    "if Testing:\n",
    "    parCore = readParamFile(False)\n",
    "    from datetime import datetime\n",
    "    lastAlert = datetime.now()\n",
    "    print(alertConditionsMet(testDF, True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flagAllAnomalies(DFpivEngScaleDist_in, verbose_in):\n",
    "    \n",
    "    import numpy as np\n",
    "    from collections import Counter\n",
    "    from numpy import unique\n",
    "    \n",
    "    if verbose_in:\n",
    "        print('Flagging all anomalies using', parCore['model_threshold_strategy'], 'strategy', end=' ')\n",
    "    \n",
    "    if parCore['model_threshold_strategy'] == 'CentroidPercentile' or \\\n",
    "       parCore['model_threshold_strategy'] == 'ScorePercentile':\n",
    "    \n",
    "        thresholdPct = float(parCore['model_threshold_value'])\n",
    "    \n",
    "    else:\n",
    "        print('ERROR: function: flagAllAnomalies. Invalid parameter value for, model_threshold_strategy')\n",
    "    \n",
    "    # For all models...\n",
    "    threshold    = np.quantile(DFpivEngScaleDist_in['distance'].to_numpy(), thresholdPct)\n",
    "    DFpivEngScaleDist_in['threshold'] = threshold\n",
    "    \n",
    "    DFpivEngScaleDist_in['anomaly'] = False\n",
    "    DFpivEngScaleDist_in.loc[DFpivEngScaleDist_in['distance'] >= DFpivEngScaleDist_in['threshold'], 'anomaly'] = True\n",
    "    \n",
    "    if verbose_in:\n",
    "        medDist = DFpivEngScaleDist_in['distance'].median().round(3)\n",
    "        maxDist = DFpivEngScaleDist_in['distance'].max().round(3)\n",
    "        print(f'TD={threshold:0.3f} medD/maxD={medDist:0.3f}/{maxDist:0.3f}')\n",
    "        print(Counter(DFpivEngScaleDist_in['anomaly']))\n",
    "    \n",
    "    return DFpivEngScaleDist_in\n",
    "\n",
    "if Testing:\n",
    "    parCore = readParamFile(False)\n",
    "    qDFpivEngScaleDistFlagged = flagAllAnomalies(qDFpivEngScaleDist, True)\n",
    "    print()\n",
    "    print('qDFpivEngScaleDistFlagged', qDFpivEngScaleDistFlagged.shape)\n",
    "    print()\n",
    "    print(qDFpivEngScaleDistFlagged[['distance','threshold','anomaly']].head(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-command",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Function: Chart Plot - anomalous or not to screen and file\n",
    "\n",
    "# pointsDF_in    - Returned DF from flagAllAnomalies function: includes points, distances, thresholds, anomaly T/F\n",
    "# dataDatesDF_in - Sample date/time of each row\n",
    "# checkrowIdx_in - Which row to check for an anomaly. Testing, could be anything, otherwise most recent\n",
    "# dim_in         - dimensionality of chart: 2 or 3\n",
    "# actionTime_in  - when the action occured, local client-side time.\n",
    "\n",
    "def chartPlot(pointsDF_in, dataDatesDF_in, checkRowIdx_in, dim_in:int, actionTime_in, verbose_in):\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    #from matplotlib import pyplot as plt\n",
    "    from datetime import datetime\n",
    "    #from datetime import date\n",
    "    \n",
    "    plotPointsDF, plotPointsAR = DimReduce(pointsDF_in.drop(columns=['distance','threshold','anomaly']), parCore['dim_reduce_model'], dim_in, verbose_in)\n",
    "    plotPointsDF['anomaly']    = pointsDF_in['anomaly']\n",
    "    \n",
    "    anomalyTrue = pointsDF_in['anomaly'].to_numpy()[checkRowIdx_in]\n",
    "    myDistance  = pointsDF_in['distance'].to_numpy()[checkRowIdx_in]\n",
    "    myThreshold = pointsDF_in['threshold'].to_numpy()[checkRowIdx_in]\n",
    "    \n",
    "    plotPointsDF['pointColor'] = 'blue'\n",
    "    plotPointsDF.loc[plotPointsDF['anomaly'] == True, 'pointColor'] = 'magenta'\n",
    "    if verbose_in:\n",
    "        print(Counter(plotPointsDF['pointColor']))\n",
    "        print(plotPointsDF.head())\n",
    "        \n",
    "    fig_size    = plt.rcParams[\"figure.figsize\"]\n",
    "    fig_size[0] = float(parCore['alert_chart_width_inch'])\n",
    "    fig_size[1] = float(parCore['alert_chart_height_inch'])\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "    \n",
    "    \n",
    "    if dim_in == 2:\n",
    "        \n",
    "        \"\"\"\n",
    "        myScoreThresholdPCT = 0.95 # INCREASE number to see LESS anoamlies... set the threshold higher\n",
    "        myScoreThresholdVAL = np.quantile(plotPointsDF['score'], myScoreThresholdPCT)\n",
    "        print('myScoreThresholdPCT, myScoreThresholdVAL', myScoreThresholdPCT, myScoreThresholdVAL)\n",
    "        \n",
    "        plotPointsDF['pointColor'] = 'blue'\n",
    "        plotPointsDF.loc[plotPointsDF['score'] > myScoreThresholdVAL, 'pointColor'] = 'red'\n",
    "        print(Counter(plotPointsDF['pointColor']))\n",
    "        print(plotPointsDF.head())\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        plt.title('Isolation Forest - Raw Points After Preprocessing & Scoring'  + '\\nScaling Into Model: ' + str(ModelScaling)+ '   Score Threshold:' + str(myScoreThresholdPCT))\n",
    "        plt.scatter(plotPointsDF[0], plotPointsDF[1], s=1, c=plotPointsDF['pointColor'])\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize 2D plot\n",
    "        fig = plt.figure()\n",
    "        \n",
    "        # Plot all points\n",
    "        plt.scatter(plotPointsDF[0], plotPointsDF[1], s=int(parCore['alert_chart_point_size_normal']), c=plotPointsDF['pointColor'])\n",
    "        #plt.scatter(plotPointsAR[:,0], plotPointsAR[:,1], s=5, c='blue')\n",
    "        \n",
    "        # Plot the point to check (normally the most recent point is what we are checking)\n",
    "        if anomalyTrue:\n",
    "            plt.scatter(plotPointsAR[checkRowIdx_in,0], plotPointsAR[checkRowIdx_in,1], s=int(parCore['alert_chart_point_size_anom']), c='red')\n",
    "        else:\n",
    "            plt.scatter(plotPointsAR[checkRowIdx_in,0], plotPointsAR[checkRowIdx_in,1], s=int(parCore['alert_chart_point_size_anom']), c='green')\n",
    "    \n",
    "    elif dim_in == 3:\n",
    "        # https://matplotlib.org/stable/tutorials/toolkits/mplot3d.html\n",
    "        #\n",
    "        # Initialize 3D plot\n",
    "        from mpl_toolkits.mplot3d import Axes3D\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.view_init(float(parCore['alert_chart_3D_elevate_angle']), float(parCore['alert_chart_3D_horizontal_angle']))\n",
    "        \n",
    "        # Plot all points\n",
    "        ax.scatter(plotPointsAR[:,0], plotPointsAR[:,1], plotPointsAR[:,2], s=int(parCore['alert_chart_point_size_normal']), c=plotPointsDF['pointColor'], marker='o')\n",
    "        ax.set_xlabel(' ')\n",
    "        ax.set_ylabel(' ')\n",
    "        ax.set_zlabel(' ')\n",
    "        \n",
    "        # Plot the point to check\n",
    "        if anomalyTrue:\n",
    "            ax.scatter(plotPointsAR[checkRowIdx_in,0], plotPointsAR[checkRowIdx_in,1], plotPointsAR[checkRowIdx_in,2], s=int(parCore['alert_chart_point_size_anom']), c='red')\n",
    "        else:\n",
    "            ax.scatter(plotPointsAR[checkRowIdx_in,0], plotPointsAR[checkRowIdx_in,1], plotPointsAR[checkRowIdx_in,2], s=int(parCore['alert_chart_point_size_anom']), c='green')\n",
    "    else:\n",
    "        print('Error in chartPlot function. Invalid dim_in value,', dim_in)\n",
    "    \n",
    "    if anomalyTrue:\n",
    "        mytitle1 = 'ANOMALY DETECTED'\n",
    "    else:\n",
    "        mytitle1 = 'Anomaly NOT Detected'\n",
    "    \n",
    "    # https://strftime.org\n",
    "    # date/time of sample based on database\n",
    "    intDT      = int(dataDatesDF_in['dt'].tail(1)) \n",
    "    sampleDT   = datetime.strptime(str(intDT), '%Y%m%d%H%M%S')\n",
    "    databaseDT = sampleDT.strftime('%d-%b-%Y %H:%M:%S')\n",
    "    \n",
    "    # single time, used for both chart title and chart png filename\n",
    "    theTimeNow = datetime.now()\n",
    "    \n",
    "    # chart also displays local time\n",
    "    localDT    = theTimeNow.strftime('%d-%b-%Y %H:%M:%S')\n",
    "    \n",
    "    mytitle2 = 'DB: {dt}   Local: {lt} '.format(dt=databaseDT, lt=localDT)\n",
    "    mytitle3 = 'Distance/Score={dist:0.3f} Threshold={thresh:0.3f}'.format(dist=myDistance, thresh=myThreshold)\n",
    "    mytitle4 = 'Model:' + parCore['model_strategy'] + ' Threshold Model:' + parCore['model_threshold_strategy'] + ' Value:' + parCore['model_threshold_value']\n",
    "    \n",
    "    plt.title(mytitle1 + '\\n' + mytitle2 + '\\n' + mytitle3 + '\\n' + mytitle4)\n",
    "    \n",
    "    if dim_in == 2:\n",
    "        chartFN = getChartFN(chartsDir2D, actionTime_in, anomalyTrue)\n",
    "    elif dim_in == 3:\n",
    "        chartFN = getChartFN(chartsDir3D, actionTime_in, anomalyTrue)\n",
    "    else:\n",
    "        print('Function chartPlot. Invalid dim_in', dim_in)\n",
    "    \n",
    "    plt.savefig(chartFN)\n",
    "    \n",
    "    if InNotebook:\n",
    "        plt.show()\n",
    "    \n",
    "    return(chartFN)\n",
    "\n",
    "if Testing:\n",
    "    parCore = readParamFile(False)\n",
    "    \n",
    "    print(\"Testing Function: chart_anom\")\n",
    "    print(qDFpiv['dt'].head())\n",
    "    \n",
    "    testrowidx = 5\n",
    "    chartPlot(qDFpivEngScaleDistFlagged, qDFpiv, testrowidx, 2, '20210309151742', True)\n",
    "    chartPlot(qDFpivEngScaleDistFlagged, qDFpiv, testrowidx, 3, '20210309151742', False)\n",
    "    print(\"Done Testing Function: chart_anom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendAnomalyDetectedEmail(chart2dFN_in, chart3dFN_in, verbose_in):\n",
    "    \n",
    "    # The actual email SEND functions are near the top of this file\n",
    "    \n",
    "    # Ensure your AnomDetect_Email.py and AnomDetect_DB.py parameters are set correctly.\n",
    "    import os\n",
    "    import AnomDetect_DB as db\n",
    "    import AnomDetect_Email as email\n",
    "    import importlib\n",
    "\n",
    "    importlib.reload(db)\n",
    "    importlib.reload(email)\n",
    "    \n",
    "    myResult = False\n",
    "    \n",
    "    subject  = 'Anomaly detected in system ' + str(db.dsn)\n",
    "    message3 = 'There has been anomaly detected in system ' + str(db.dsn) + ' at ' + datetime.now().strftime(\"%d-%b-%Y %H:%M:%S\")\n",
    "    message4 = '2D chart filename: ' + str(chart2dFN_in)\n",
    "    message5 = '3D chart filename: ' + str(chart3dFN_in)\n",
    "    message2 = 'Anomaly Detection System: ' + str(lastCodeUpdate)\n",
    "    message1 = 'From: ' + str(email.sender_email)\n",
    "    \n",
    "    message  = str(message1 + '\\n' + message2 + '\\n\\n' + message3 + '\\n' + '\\n' + message4 + '\\n' + message5)\n",
    "    \n",
    "    myLine   = 'Sending alert email from ' + str(email.sender_email) + ' to ' + str(email.receiver_email)\n",
    "    \n",
    "    print(myLine, end='...')\n",
    "    \n",
    "    if verbose_in:\n",
    "        print('\\nMessage is:')\n",
    "        print(message, '\\n')\n",
    "    \n",
    "    if not verbose_in:\n",
    "        if str2bool(parCore['alert_email_wo_charts']):       \n",
    "            myResult = sendAlertEmail_wo_charts(subject, message, True)\n",
    "        \n",
    "        if str2bool(parCore['alert_email_w_charts']):\n",
    "            myFN = os.path.basename(chart2dFN_in)\n",
    "            myResult = sendAlertEmail_w_charts(subject, message, myFN, True)\n",
    "    \n",
    "    if myResult:\n",
    "        myLineEnd = ' was successfully sent.'\n",
    "    else:\n",
    "        myLineEnd = ' was NOT successfully sent.'\n",
    "\n",
    "    print(myLineEnd)\n",
    "    alertLogWrite(myLine + myLineEnd)\n",
    "    \n",
    "if Testing:\n",
    "    print('Testing sendTextEmail')\n",
    "    # verbose=True will NOT send message, but will show message text\n",
    "    sendAnomalyDetectedEmail('Tester 2D filename', 'Tester 3D filename', True)\n",
    "\n",
    "print('Done.')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-puppy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def anomalyAction(fullDF_in, pivDF_in, verbose_in):\n",
    "    \n",
    "    from datetime import datetime\n",
    "    \n",
    "    myDebug = False\n",
    "    \n",
    "    # This is THE time of the action. It is used for many things, like: chart filename, header, local time\n",
    "    actionTime = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    if verbose_in:\n",
    "        print('anomalyAction function inputs:')\n",
    "        print('fullDF_in, pivDF_in, verbose_in')\n",
    "        print(fullDF_in.shape, pivDF_in.shape, verbose_in)\n",
    "    \n",
    "    forceAnomalyTrue = str2bool(parCore['debug_force_anomaly_enable'])\n",
    "    \n",
    "    mostRecentRow    = fullDF_in.shape[0]-1\n",
    "    isAnomalyTrue    = forceAnomalyTrue or fullDF_in['anomaly'].to_numpy()[mostRecentRow]\n",
    "    myDistance       = fullDF_in['distance'].to_numpy()[mostRecentRow]\n",
    "    myThreshold      = fullDF_in['threshold'].to_numpy()[mostRecentRow]\n",
    "    \n",
    "    medDist          = fullDF_in['distance'].median()\n",
    "    maxDist          = fullDF_in['distance'].max()\n",
    "    \n",
    "    modStrat = parCore['model_strategy']\n",
    "    \n",
    "    stats = f'RID={mostRecentRow} D={myDistance:0.2f} T={myThreshold:0.2f} {isAnomalyTrue} medD/maxD={medDist:0.2f}/{maxDist:0.2f} {modStrat}'\n",
    "    \n",
    "    if str2bool(parCore['alert_console']) or verbose_in:\n",
    "        line = 'Anomaly '\n",
    "        if isAnomalyTrue != True:\n",
    "            line = line + 'NOT detected'\n",
    "        else:\n",
    "            line = line + 'DETECTED'\n",
    "        if forceAnomalyTrue:\n",
    "            line = line + ' but FORCED'\n",
    "    \n",
    "        print(line, stats)\n",
    "\n",
    "    if str2bool(parCore['alert_log_enable']) or verbose_in:\n",
    "        alertLogWrite(line + ', ' + stats)\n",
    "        \n",
    "    # alert_chart must be True or no emails will be sent\n",
    "    \n",
    "    if str2bool(parCore['model_chart_display_always']) or \\\n",
    "       (isAnomalyTrue and str2bool(parCore['alert_chart'])) or \\\n",
    "       verbose_in:\n",
    "        \n",
    "        chart2dFN = chartPlot(fullDF_in, pivDF_in, mostRecentRow, 2, actionTime, verbose_in)\n",
    "        chart3dFN = chartPlot(fullDF_in, pivDF_in, mostRecentRow, 3, actionTime, verbose_in)\n",
    "        \n",
    "        if (str2bool(parCore['alert_email_w_charts']) or str2bool(parCore['alert_email_wo_charts'])):\n",
    "            \n",
    "            sendAnomalyDetectedEmail(chart2dFN, chart3dFN, verbose_in)\n",
    "            \n",
    "        else:\n",
    "            if verbose_in:\n",
    "                print('Not sending an alert email.')\n",
    "        \n",
    "    if (isAnomalyTrue and str2bool(parCore['alert_log_enable'])) or myDebug:\n",
    "        \n",
    "        myFN2D = getChartFN(chartsDir2D, actionTime, isAnomalyTrue)\n",
    "        myFN3D = getChartFN(chartsDir3D, actionTime, isAnomalyTrue)\n",
    "        \n",
    "        myLine = 'Alert triggered, sample=, ' + str(mostRecentRow) + \\\n",
    "              ', distance=, ' + str(round(myDistance,2)) + ', threshold=, ' + str(round(myThreshold,2)) + \\\n",
    "              ', 2D=, ' + str(myFN2D) + ', 3D=, ' + str(myFN3D) + '\\n'\n",
    "        \n",
    "        alertLogWrite(myLine)\n",
    "        \n",
    "        if verbose_in:\n",
    "            print('\\n\\n' + myLine + '\\n\\n')\n",
    "    \n",
    "    return(isAnomalyTrue) # return isAnomalyTrue is needed to set the lastAlert\n",
    "\n",
    "if Testing:\n",
    "    print(\"Testing Function: anomalyAction\")\n",
    "    print('qDFpivEngScaleDistFlagged', qDFpivEngScaleDistFlagged.shape, 'qDFpiv', qDFpiv.shape)\n",
    "    anomalyAction(qDFpivEngScaleDistFlagged, qDFpiv, True)\n",
    "\n",
    "print('Done')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doCommandLineChecks():\n",
    "    \n",
    "    if InNotebook:\n",
    "        if CmdCheck == 'db':\n",
    "            print('\\nConnection test result:', checkConnectAndQuery(False))\n",
    "            print('\\nYou may seen an exception message below, that can be ignored.')\n",
    "            doExit()\n",
    "        elif CmdCheck == 'email':\n",
    "            print('\\nEmail test result:', sendAlertEmail_wo_charts('Test subject line', 'This is a test message', False))\n",
    "            print('Check your inbox.')\n",
    "            print('\\nYou may seen an exception message below, that can be ignored.')\n",
    "            doExit()\n",
    "    else:\n",
    "        if len(sys.argv) > 1 and str(sys.argv[1]) == 'db':\n",
    "            print('\\nConnection test result:', checkConnectAndQuery(False))\n",
    "            doExit()\n",
    "        elif len(sys.argv) > 1 and str(sys.argv[1]) == 'email':\n",
    "            print('\\nEmail test result:', sendAlertEmail_wo_charts('Test subject line', 'This is a test message', False))\n",
    "            print('Check your inbox.')\n",
    "            doExit()\n",
    "\n",
    "if Testing:\n",
    "    doCommandLineChecks()\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-mumbai",
   "metadata": {},
   "source": [
    "# Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-people",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import AnomDetect_DB as db\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "line = str('Starting Main Program at ' + datetime.now().strftime(\"%d-%b-%Y %H:%M:%S\") + ' Last update ' + lastCodeUpdate)\n",
    "print(line, '\\n')\n",
    "alertLogWrite(line)\n",
    "\n",
    "parCore = readParamFile(True)\n",
    "print()\n",
    "\n",
    "doCommandLineChecks()\n",
    "\n",
    "# set global verbose variable that any function can use\n",
    "mainVerbose = str2bool(parCore['debug_detail_enable'])\n",
    "\n",
    "# Email Alerting: Ensure your AnomDetect_Email.py parameters are set correctly.\n",
    "import AnomDetect_Email as email\n",
    "line = parCore['alert_email_w_charts'] + ', alert emails with CHARTS are enabled'\n",
    "print(line)\n",
    "alertLogWrite(line)\n",
    "line = parCore['alert_email_wo_charts'] + ', alert emails with NO CHARTS are enabled'\n",
    "print(line)\n",
    "alertLogWrite(line)\n",
    "line = 'Emails are sent from ' + str(email.sender_email) + ' to ' + str(email.receiver_email)\n",
    "print(line, '\\n')\n",
    "alertLogWrite(line)\n",
    "\n",
    "# create our inital core dataframe\n",
    "coreDF = defineCoreDF(mainVerbose)\n",
    "rawDF  = defineCoreDF(mainVerbose)\n",
    "\n",
    "# Perform basic DB and Query check before entering main loop\n",
    "check1 = checkConnectAndQuery(mainVerbose)\n",
    "check2, dbConnection = makeDbConnection(show_error_in=True,verbose_in=True)\n",
    "\n",
    "if check1 and check2:\n",
    "    haveRestartData, restartDataDF, restartOldestDt, restartNewestDt = loadRestartData(mainVerbose, True)\n",
    "\n",
    "    if haveRestartData:\n",
    "        print('Restart data loaded from', restartOldestDt, 'to', restartNewestDt, restartDataDF.shape[0], 'rows')\n",
    "        rawDF = restartDataDF.copy()\n",
    "        rawDF = resetCoreDFtypes(rawDF, mainVerbose)\n",
    "    else:\n",
    "        print('Restart data not available.')\n",
    "    \n",
    "    # Alerting can be disabled, while data is still being collected.\n",
    "    print('Alerting is enabled,', parCore['alert_enable'], 'and minimum sample sets at', parCore['alert_min_sample_sets'])\n",
    "    \n",
    "    print('Starting collection at ' + datetime.now().strftime(\"%d-%b-%Y %H:%M:%S\"))\n",
    "    \n",
    "    lastAlert      = datetime.today() - timedelta(days=1)\n",
    "    lastCheckpoint = datetime.now()\n",
    "    \n",
    "    # THE MAIN LOOP\n",
    "    \n",
    "    while True:\n",
    "        mainVerbose = str2bool(parCore['debug_detail_enable'])\n",
    "        \n",
    "        print('\\n' + datetime.now().strftime(\"%d-%b %H:%M:%S\"), end=' ' )\n",
    "        samplesBefore = rawDF.shape[0]\n",
    "        \n",
    "        newSamplesLoaded, rawDF = addNewRows(rawDF, mainVerbose)\n",
    "        \n",
    "        if newSamplesLoaded:\n",
    "            \n",
    "            #print('\\n1 coreDF.shape, rawDF.shape',coreDF.shape, rawDF.shape)\n",
    "            coreDF = rawDF.tail(int(parCore['data_max_samples_modeled'])).copy()\n",
    "            coreDF = resetCoreDFtypes(coreDF, mainVerbose)\n",
    "            #print('2', coreDF['dt'].min(), coreDF['dt'].max())\n",
    "            coreDF = coreDF[coreDF['dt'] > coreDF['dt'].min()].copy()\n",
    "                    \n",
    "            #print('3 coreDF.shape, rawDF.shape',coreDF.shape, rawDF.shape)\n",
    "            #print('4', coreDF['dt'].min(), coreDF['dt'].max())\n",
    "            \n",
    "            samplesAfter = rawDF.shape[0]\n",
    "            \n",
    "            if (samplesAfter > samplesBefore) and alertConditionsMet(coreDF, mainVerbose):\n",
    "                print(' coreDF', coreDF.shape, end='')\n",
    "                if str2bool(parCore['data_max_samples_show_details']):\n",
    "                    print('\\n                coreDF.dt oldest/newest ' + str(coreDF['dt'].min()) + \\\n",
    "                          '/' + str(coreDF['dt'].max()), end='' )\n",
    "                \n",
    "                DFpiv, DFpivEng, DFpivEngScale, DFpivEngScaleDist = preprocessCoreDF(coreDF, mainVerbose, True)\n",
    "                DFpivEngScaleDistFlagged                          = flagAllAnomalies(DFpivEngScaleDist, mainVerbose)\n",
    "                if anomalyAction( DFpivEngScaleDistFlagged, DFpiv, mainVerbose ):\n",
    "                    lastAlert = datetime.today()\n",
    "        \n",
    "        if checkDBFileChange():\n",
    "                check2, dbConnection = makeDbConnection(show_error_in=True,verbose_in=True)\n",
    "        \n",
    "        time.sleep(int(parCore['sample_frequency_sec']))\n",
    "        \n",
    "        parCore        = readParamFile(False)\n",
    "        lastCheckpoint = checkpoint(lastCheckpoint, rawDF, False)\n",
    "    \n",
    "else:\n",
    "    doExit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-swift",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-klein",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
